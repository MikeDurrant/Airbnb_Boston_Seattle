{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A data tale of two cities that will make you a better Airbnb host\n### Airbnb review rating analysis of the Seattle and Boston datasets from 2016/17 for Udacity Data Science Nanodegree Blog Post Task"},{"metadata":{},"cell_type":"markdown","source":"# Intro\n\nReviews are of enormous importance in online marketplaces, nowhere more so than for Airbnb hosts.  Guests are potentially taking a greater risk than if they booked into a well known hotel chain, so the rating for an Airbnb property can give comfort or scare away guests in equal measure.\n\nWhat actionable steps can hosts take to make sure they are maximising their overall review scores and avoiding the simple mistakes and oversights that can lead to damaging negative reviews?  And, is this the same story whether the property is in Seattle or Boston?\n\nThis analysis dives deep into the datasets of both cities to shed light on this important question.  Spoiler alert - there are some very simple steps that can be taken that might make a big improvement in overall review scores.  They might be obvious to those who already do them right, but there are a lot of hosts who would do well to take note and make a few simple changes for some great improvements in their ratings."},{"metadata":{},"cell_type":"markdown","source":"# Context\n\nA key overall measure for review ratings is 'review_scores_rating'.  It is a measure out of 100 and gives an overall rating score for each property listing.\n\nHelpfully there are a number of sub-category review scores (e.g. 'value', 'cleanliness') which are measured out of 10, and this analysis looks deeper into those to see if any of them give clues for how to unlock a higher 'review_scores_rating' overall.\n\nThere were many other possible avenues of investigation in these datasets.  After some initial exploratory data analysis, this workbook focusses in on the following three business questions for Airbnb hosts.\n\nThis project has been prepared as part of my coursework for the Udacity Data Science nanodegree and has an accompanying blog post on Medium.  (Link - https://mikedurrantsheffield.medium.com/a-data-tale-of-two-cities-that-will-make-you-a-better-airbnb-host-48a154b911ee).  The data uses the Boston and Seattle Airbnb datasets from Kaggle which contain 7,403 property listings in total from 2016/17.\n\nThe data has been handled following the CRISP-DM process from understanding the business questions and understanding the data, through to communicating answers to the problems using clear visualisations and the Medium blog post linked above.\n\n\n\n## Question 1\n### What actions can a host take to achieve a better overall rating score?\n\n\n## Question 2\n### From the sub-category review scores, which ones have the strongest impact on overall rating score?\n\n\n\n## Question 3\n### How do the drivers of overall rating differ by city between Seattle and Boston?"},{"metadata":{},"cell_type":"markdown","source":"# Load in the libraries and files (Gather)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Load in the Python libraries required for this analysis\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Kaggle os set up - code not necessary if not importing files direct from Kaggle\n\n#\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframes for the six csv data tables\n\ndf_seattle_calendar = pd.read_csv(r'/kaggle/input/seattle/calendar.csv')\ndf_seattle_listings = pd.read_csv(r'/kaggle/input/seattle/listings.csv')\ndf_seattle_reviews = pd.read_csv(r'/kaggle/input/seattle/reviews.csv')\ndf_boston_calendar = pd.read_csv(r'/kaggle/input/boston/calendar.csv')\ndf_boston_listings = pd.read_csv(r'/kaggle/input/boston/listings.csv')\ndf_boston_reviews = pd.read_csv(r'/kaggle/input/boston/reviews.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the shape of the listings df and see if the two cities match up\n\ndf_seattle_listings.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_boston_listings.shape # 3 additional columns in Boston","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(df_boston_listings.columns) - set(df_seattle_listings.columns)  # these are the three extra columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(df_seattle_listings.columns) - set(df_boston_listings.columns)  # no extra columns in Seattle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add city column to the listings to differentiate\ndf_boston_listings['city2'] = 'BOSTON'  \ndf_seattle_listings['city2'] = 'SEATTLE'  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate listings tables\ndf_list = pd.concat([df_seattle_listings, df_boston_listings])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list.shape # new df concat includes all 96 columns from Boston; the 3 'Boston-only' columns will not be useful in city v city comparisons","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add city column to the calendar to differentiate\ndf_boston_calendar['city'] = 'BOSTON'  \ndf_seattle_calendar['city'] = 'SEATTLE'  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate calendar tables\ndf_cal = pd.concat([df_seattle_calendar, df_boston_calendar])\ndf_cal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_boston_reviews.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_seattle_reviews.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add city column to the reviews tables to differentiate\ndf_boston_reviews['city'] = 'BOSTON'\ndf_seattle_reviews['city'] = 'SEATTLE'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate review tables\ndf_revs = pd.concat([df_seattle_reviews, df_boston_reviews])\ndf_revs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Begin Exploratory Data Analysis (Assess)\n"},{"metadata":{},"cell_type":"markdown","source":"## CALENDAR table"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calendar table is a list of availability and price data\ndf_cal ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Possible business questions emerging part 1 (for long list; to be narrowed later):\n\n* What factors drive the popularity of a property - as indicated by % fully booked?\n* What factors affect the price that a property is listed at - based on listing prices (either including the date to bring in seasonality as a factor; or using a fixed date period average as the response variable to avoid seasonality)?\n* What dates are particularly popular for this district/city (to bring in Boston data as well to join and compare) in terms of booking availability and price advertised?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cal.shape   # it has just over 2.7 million rows and 5 columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cal[df_cal['city'] == 'BOSTON'].shape  # 1.3 million rows for Boston","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cal[df_cal['city'] == 'SEATTLE'].shape  # 1.4 million rows for Seattle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cal.listing_id.nunique()  # it covers 7,403 unique listing id's so 7,403 unique rental properties","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_cal.date.max())  # the calendar starts on 4th Jan 2016\nprint(df_cal.date.min())  # the latest date listed is 5th Sep 2017","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_cal[df_cal['city'] == 'SEATTLE'].date.min())\nprint(df_cal[df_cal['city'] == 'SEATTLE'].date.max())\nprint(df_cal[df_cal['city'] == 'BOSTON'].date.min())\nprint(df_cal[df_cal['city'] == 'BOSTON'].date.max())\n\n# slightly overlapping periods in the data\n# both cities have approximately one year's worth\n# boston data is very slightly more recent\n# this should not give us any problems as the periods are similar and cover a year, but something to keep in mind in all analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cal.isnull().sum()/df_cal.shape[0]    # around 42% of rows have the price missing; no missing values from other columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cal_avail = df_cal[df_cal['available']=='t']  # for rows where the date and listing_id are showing as available there are no missing price\n\ndf_cal_avail.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cal_unavail = df_cal[df_cal['available']=='f']  # for rows where the date and listing_id are showing as available there are 100% missing prices\n\ndf_cal_unavail.isnull().sum()/df_cal_unavail.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MISSING VALUES - for the calendar table this is all clear, with no imputing or adjusting required.  The table includes price for all properties on dates that are showing as available and does not include price for any dates that are showing as unavailable."},{"metadata":{},"cell_type":"markdown","source":"## LISTINGS table"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list   # wide table with 96 columns of information about each listing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Possible business questions emerging part 2 (for long list; to be narrowed later):\n* REVIEW_SCORES_RATING as response variable\n\n    * What factors have the greatest impact on the review scores rating?\n    * Does host response time affect review scores rating?\n    * Does length of text written in description affect review scores rating?\n    * Does host profile affect review score?  E.g. has_profile_pic, host_about length of text, host_verified etc.\n    * Does amenities affect review score rating?  Number of amenities?  Categories of amenities?\n    * Does price/cleaning fee etc. affect review score?\n    * How does cancellation policy affect review score?\n    * Which sub-review scores (i.e. accuracy, cleanliness, checkin etc. have greatest impact on overall review scores rating?\n    * How do review scores differ between the two cities?\n    \n    \n    \n* CALENDAR_PRICE as response variable\n\n    * What factors affect the price listed?\n    * What price should a new property list at?\n    * How does location affect price?\n    * How do amenities affect price?\n    * How do reviews or host status affect price?\n    * How does price differ between the two cities?\n    * Are there different drivers of price between the two cities?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list.shape # 7,403 rows matches the 7,403 figure for unique listings in the calendar table; 96 cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list['state'].value_counts()   # shows that we need to use the city2 column for split of dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list.columns  \n# full list of 96 column names; most seem self explanatory; want to understand review_scores_rating to see if it is useful overall response variable\n# remember last three columns are not present for the Seattle dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to plot some histograms for EDA\n\ndef plot_hists(columnlist):\n    \"\"\"\n    INPUT\n    A list of columns that we want to see side by side histograms for\n    \n    OUTPUT\n    Histograms on shared axes for the two cities\n    \"\"\"\n    \n    for col in columnlist:\n        boston = df_list[df_list['city2'] == 'BOSTON'][col]\n        seattle = df_list[df_list['city2'] == 'SEATTLE'][col]\n        plt.hist([boston, seattle], label=['Boston', 'Seattle']) \n        plt.title(col)\n        plt.legend()\n        plt.show()\n        \ncolumnlist = ['accommodates', 'beds', 'bathrooms', 'price', 'square_feet', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness']\nplot_hists(columnlist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pivot tables for the categorical variables - impact on overall rating\n\npd.pivot_table(df_list, values='review_scores_rating', index='host_response_time', columns='city2', aggfunc='mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## REVIEWS table"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_revs # text based view of the reviews left by guests","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Possible business questions emerging part 3 (for long list; to be narrowed later):\n* REVIEW_SCORES_RATING as response variable\n\n    * What keywords from the comments stand out as being linked to high and low review scores?\n    * What keywords from the comments stand out as difference between the two cities?\n    \n    \n    \n* CALENDAR_PRICE as response variable\n\n    * What keywords from the comments stand out as being linked to high and low price listings?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_revs.shape  # nearly 153 thousand rows of reviews for the 7 columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_revs.listing_id.nunique() # based on 6,020 unique listing id's - this lines up with the number of listings that have more than zero reviews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_revs.listing_id.nunique()/df_list.shape[0]  # shows us that 81.3% of listings have at least one review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list_with_reviews = df_list[df_list.number_of_reviews > 0]  # check how many listings have more than zero reviews\ndf_list_with_reviews.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Framing the Business Questions"},{"metadata":{},"cell_type":"markdown","source":"## Question 1\n\nWhat actions can a host take to achieve a better overall rating score?"},{"metadata":{},"cell_type":"markdown","source":"## Question 2\n\nFrom the sub-category review scores, which ones have the strongest impact on overall rating score?"},{"metadata":{},"cell_type":"markdown","source":"## Question 3\n\nHow do the drivers of overall rating differ by city between Seattle and Boston?"},{"metadata":{},"cell_type":"markdown","source":"#### NB there are clearly a vast number of other questions that could be asked of this dataset.  Some of these were suggested in the EDA phase and could be explored at another time, but for this analysis we will focus on the three questions above only."},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning Stage (Clean)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list.isnull() # a matrix showing which data has null or NaN values that will need removing or imputing before a model is run","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# understand the split of categorical and numerical variables\n\ndf_list.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of datatypes\n\ndf_list.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of the numerical variables columns to use later\n\nlist_num_vars = list(df_list.select_dtypes(include=['int64', 'float64']).columns)\n\nlist_num_vars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of the categorical columns to use later\n\nlist_cat_vars = list(df_list.select_dtypes(include=['object']).columns)\nlist_cat_vars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many rows for the response variable have errors\n\ndf_list.review_scores_rating.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# what proportion of the dataset does this affect\n\ndf_list.review_scores_rating.isnull().sum() / df_list.shape[0]\n\n# nearly 20% have no figure for review_scores_rating - but these all need to be removed as the response variable must not be blank and should not be imputed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create new df list clean that removes the empty rows for response variable\n\ndf_list_clean = df_list.dropna(subset=['review_scores_rating'], axis=0)\ndf_list_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(7403-1460)\ndf_list_clean.shape # checking this df is shorter than the original df by 1,460 rows.  It has 5,943 rows so this is correct.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many nulls\n\npd.set_option('display.max_rows',100)\ndf_list_clean.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check nulls by % of total (filtered where more than 30% of rows are nulls)\n\ncols_to_drop = (df_list_clean.isnull().sum()/df_list_clean.shape[0])[df_list_clean.isnull().sum()/df_list_clean.shape[0] > 0.3]\ncols_to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of these cols to drop\n\nlist_cols_to_drop = list(cols_to_drop.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop these columns\n\ndf_list_clean = df_list_clean.drop(list_cols_to_drop, axis=1)\ndf_list_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check nulls by % of total\n\ndf_list_clean.isnull().sum() / df_list_clean.shape[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill mean values to replace errors for all numerical columns\n\nfill_mean = lambda col: col.fillna(col.mean()) if col.name in list_num_vars else col       # lambda function to apply to all num_var columns with missing values\n\ndf_list_clean = df_list_clean.apply(fill_mean)\n\ndf_list_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get dummies for categorical variables, but only if fewer than 50 categories\n\nlist_cat_cols_below50 = []   # blank list to capture cat cols below 50 categories\n\nfor i in df_list_clean.filter(list_cat_vars).columns:\n    if len(df_list_clean[i].value_counts()) < 50:\n        list_cat_cols_below50.append(i)\n        \nlist_cat_cols_below50\n\ndf_list_clean = pd.get_dummies(df_list_clean, columns=list_cat_cols_below50, dummy_na=True)\n\ndf_list_clean\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look at remaining null row proportions to decide whether to clean or drop\n\npd.set_option('display.max_rows', 300)\n(df_list_clean.isnull().sum() / df_list_clean.shape[0]) [df_list_clean.isnull().sum() / df_list_clean.shape[0] > 0]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from the list of remaining nulls - these columns will be dropped as they would likely add little value to the model\n\ncols_to_drop = ['summary', 'neighborhood_overview', 'transit', 'thumbnail_url', 'medium_url', 'xl_picture_url', 'host_about']\n\ndf_list_clean = df_list_clean.drop(cols_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remaining columns with nulls\n\ndf_list_clean.isnull().sum() [df_list_clean.isnull().sum() >0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for remaining columns, which might be of use, we will fill with mode\n\nmode_fill_cols = list((df_list_clean.isnull().sum() [df_list_clean.isnull().sum() >0]).index)\n\nfill_mode = lambda col: col.fillna(col.mode()[0]) if col.name in mode_fill_cols else col    # lambda function to apply to all mode fill columns with missing values\n\ndf_list_clean = df_list_clean.apply(fill_mode)\n\ndf_list_clean\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if all columns are now showing zero nulls - ready for modelling\n\nnp.sum(df_list_clean.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further analysis before modelling (Analyze)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking at pre-cleaned df how does each factor appear to affect the response variable - superhost\n\npd.pivot_table(df_list, values='review_scores_rating', columns='city2', index='host_is_superhost', aggfunc='mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Super Host status appears to be strongly linked to higher rating - but this could be self-fulfilling (ie. to become a superhost a high rating is required).  Seattle non-superhosts score better than Boston."},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking at pre-cleaned df how does each factor appear to affect the response variable - host has profile pic\n\npd.pivot_table(df_list, values='review_scores_rating', columns='city2', index='host_has_profile_pic', aggfunc='mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Profile pic seems to be negatively related to review scores - this is unexpected and may impact the model.  Same pattern in both cities - Seattle no profile pic hosts have very high ratings."},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking at pre-cleaned df how does each factor appear to affect the response variable - host response time\n\npd.pivot_table(df_list, values='review_scores_rating', columns='city2', index='host_response_time', aggfunc='mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not too much impact as long as response at least within a day, but longer than that seems to have a large penalty - the penalty being much greater in Seattle."},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking at pre-cleaned df how does each factor appear to affect the response variable - room type\n\npd.pivot_table(df_list, values='review_scores_rating', columns='city2', index='room_type', aggfunc='mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shared room very slightly lower scores in both cities.  Otherwise not appearing to be a strong factor."},{"metadata":{},"cell_type":"markdown","source":"# Modelling (Model)"},{"metadata":{},"cell_type":"markdown","source":"\n## A linear regression model will be used to see what predictions can be made for Review Rating Score and what factors drive this.  First using the data overall and then looking at each city individually."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import sklearn libraries\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list_clean.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef run_pipeline(feature_list):\n    \"\"\"\n    Creates a pipeline to generate r2 scores on a linear regression model, enabling iteration over multiple feature lists\n    \n    INPUT\n    A single feature list of any length that we want to use as the X dataframe for explanatory variables\n    \n    OUTPUT\n    A single R Squared score for the accuracy of the y_test_preds against the y_test dataset\n    \n    \"\"\"\n\n    # split dataframe in X and y explanatory and response variables\n\n    X = df_list_clean.drop(columns=['review_scores_rating'], axis=1)\n    y = df_list_clean['review_scores_rating']\n    \n    # trim X down to feature list only\n    \n    \n    X = X[feature_list]      # for the feature_list argument passed to this function\n    \n        \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=42)    # split into train and test\n\n    lm_model = LinearRegression(normalize=True)    # instantiate model\n\n    lm_model.fit(X_train, y_train)    # fit model\n\n    y_test_preds = lm_model.predict(X_test)    # predict using the model\n\n    score = r2_score(y_test, y_test_preds)      # score the model\n    \n    return score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explanatory note - for the building of the feature matrix below I ran through a few iterations and manually appended the best performing feature to the list.  Each iteration works out the best item for 'i' which is the next to append to the list.  This is more manual than I would like, but it helped me to work through each example and see how the feature list could be tuned by adding on item each iteration."},{"metadata":{"trusted":true},"cell_type":"code","source":"list(df_list_clean.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate feature matrix (list of lists)\n\nnumeric_cols = df_list_clean.select_dtypes(include=['int64', 'uint8', 'float64'])   # create a df of the numeric cols\nnumeric_cols = numeric_cols.drop(columns=['review_scores_rating'], axis=1)      # drop the response variable from this df\n\nnum_cols_list = list(numeric_cols.columns)     # turn the df into a list of column names\n\nfeature_matrix = []         \n\n\n# for loop to generate a list of lists - for each column for 'i' - to be passed to the pipeline for scoring\n\nfor i in num_cols_list[1:]:                                  \n    feature_matrix.append(\n                            [\n                            'review_scores_value', \n                            'review_scores_cleanliness', \n                            'review_scores_checkin', \n                            'review_scores_location', \n                            'review_scores_accuracy',\n                            'reviews_per_month',\n                            'host_identity_verified_t',\n                            'review_scores_communication',\n                            'bedrooms',\n                            'room_type_Entire home/apt',\n                            'property_type_Loft',\n                            'calendar_updated_4 months ago',\n                            'calendar_updated_4 weeks ago',\n                            'require_guest_phone_verification_t',\n                            'calendar_updated_2 days ago',\n                            'property_type_Other',\n                            'calendar_updated_7 months ago',\n                            'host_listings_count',\n                            'host_response_time_a few days or more',\n                            i,\n                            ]\n                        )\n\nfeature_matrix\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dataframe ready to capture scores from the feature list and populate that df with the feature list and the score\n\noutputs = pd.DataFrame({'feature_list':[], 'score':[]})\n\ni=0\n\nfor n in feature_matrix:\n    score = run_pipeline(n)\n    outputs.loc[i] = np.array([n, score], dtype=object)\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the feature list with the highest r squared score from the test\n\nlist(outputs.sort_values('score', ascending=False).head(1).feature_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the scoring dataframe, sorted by highest score first\n\noutputs.sort_values('score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run model once for highest performing feature set\n\nfeature_list_best = ['review_scores_value', \n                'review_scores_cleanliness', \n                'review_scores_checkin', \n                'review_scores_location', \n                'review_scores_accuracy',\n                'reviews_per_month',\n                'host_identity_verified_t',\n                'review_scores_communication',\n                'bedrooms',\n                'room_type_Entire home/apt',\n                'property_type_Loft',\n                'calendar_updated_4 months ago',\n                'calendar_updated_4 weeks ago',\n                'require_guest_phone_verification_t',\n                'calendar_updated_2 days ago',\n                'property_type_Other',\n                'calendar_updated_7 months ago',\n                'host_listings_count',\n                'host_response_time_a few days or more',\n               ]\n\n\n\n# split dataframe in X and y explanatory and response variables\n\nX_best = df_list_clean.drop(columns=['review_scores_rating'], axis=1)\ny = df_list_clean['review_scores_rating']\n\n# trim X down to feature list only\n\n\nX_best = X_best[feature_list_best]      # for the feature_list argument passed to this function\n\n\nX_best_train, X_best_test, y_train, y_test = train_test_split(X_best, y, test_size=.30, random_state=42)    # split into train and test\n\nlm_model = LinearRegression(normalize=True)    # instantiate model\n\nlm_model.fit(X_best_train, y_train)    # fit model\n\ny_test_preds = lm_model.predict(X_best_test)    # predict using the model\n\nscore = r2_score(y_test, y_test_preds)      # score the model\n\nprint(score)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# which features have a stronger weighting on the model - using coefficients?\n\ndef coef_weights(coefficients, X_train):\n    '''\n    INPUT:\n    coefficients - the coefficients of the linear model \n    X_train - the training data, so the column names can be used\n    OUTPUT:\n    coefs_df - a dataframe holding the coefficient, estimate, and abs(estimate)\n    \n    Provides a dataframe that can be used to understand the most influential coefficients\n    in a linear model by providing the coefficient estimates along with the name of the \n    variable attached to the coefficient.\n    '''\n    coefs_df = pd.DataFrame()\n    coefs_df['est_int'] = X_train.columns\n    coefs_df['coefs'] = lm_model.coef_\n    coefs_df['abs_coefs'] = np.abs(lm_model.coef_)\n    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n    return coefs_df\n\n#Use the function\ncoef_df = coef_weights(lm_model.coef_, X_best_train)\n\n#A quick look at the top results\ncoef_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(df_list, index='review_scores_value', columns='city2', values='review_scores_rating', aggfunc='mean').sort_values('review_scores_value', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(df_list, index='review_scores_cleanliness', columns='city2', values='review_scores_rating', aggfunc='mean').sort_values('review_scores_cleanliness', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list_clean[df_list_clean.city2_BOSTON == True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run model once for highest performing feature set - BOSTON\n\nfeature_list_best = ['review_scores_value', \n                'review_scores_cleanliness', \n                'review_scores_checkin', \n                'review_scores_location', \n                'review_scores_accuracy',\n                'reviews_per_month',\n                'host_identity_verified_t',\n                'review_scores_communication',\n                'bedrooms',\n                'room_type_Entire home/apt',\n                'property_type_Loft',\n                'calendar_updated_4 months ago',\n                'calendar_updated_4 weeks ago',\n                'require_guest_phone_verification_t',\n                'calendar_updated_2 days ago',\n                'property_type_Other',\n                'calendar_updated_7 months ago',\n                'host_listings_count',\n                'host_response_time_a few days or more',\n               ]\n\n\n# split data frame to be only Boston\n\ndf_boston = df_list_clean[df_list_clean.city2_BOSTON == True]\n\n\n# split dataframe in X and y explanatory and response variables\n\nX_boston = df_boston.drop(columns=['review_scores_rating'], axis=1)\ny_boston = df_boston['review_scores_rating']\n\n# trim X down to feature list only\n\n\nX_best_boston = X_boston[feature_list_best]      # for the feature_list argument passed to this function\n\n\nX_best_boston_train, X_best_boston_test, y_boston_train, y_boston_test = train_test_split(X_best_boston, y_boston, test_size=.30, random_state=42)    # split into train and test\n\nlm_model_boston = LinearRegression(normalize=True)    # instantiate model\n\nlm_model_boston.fit(X_best_boston_train, y_boston_train)    # fit model\n\ny_boston_test_preds = lm_model_boston.predict(X_best_boston_test)    # predict using the model\n\nscore = r2_score(y_boston_test, y_boston_test_preds)      # score the model\n\nprint(score)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# which features have a stronger weighting on the model - BOSTON - using coefficients?\n\n#Use the function\ncoef_df_boston = coef_weights(lm_model_boston.coef_, X_best_boston_train)\n\n#A quick look at the top results\ncoef_df_boston.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run model once for highest performing feature set - SEATTLE\n\nfeature_list_best = ['review_scores_value', \n                'review_scores_cleanliness', \n                'review_scores_checkin', \n                'review_scores_location', \n                'review_scores_accuracy',\n                'reviews_per_month',\n                'host_identity_verified_t',\n                'review_scores_communication',\n                'bedrooms',\n                'room_type_Entire home/apt',\n                'property_type_Loft',\n                'calendar_updated_4 months ago',\n                'calendar_updated_4 weeks ago',\n                'require_guest_phone_verification_t',\n                'calendar_updated_2 days ago',\n                'property_type_Other',\n                'calendar_updated_7 months ago',\n                'host_listings_count',\n                'host_response_time_a few days or more',\n               ]\n\n\n# split data frame to be only Seattle\n\ndf_seattle = df_list_clean[df_list_clean.city2_SEATTLE == True]\n\n\n# split dataframe in X and y explanatory and response variables\n\nX_seattle = df_seattle.drop(columns=['review_scores_rating'], axis=1)\ny_seattle = df_seattle['review_scores_rating']\n\n# trim X down to feature list only\n\n\nX_best_seattle = X_seattle[feature_list_best]      # for the feature_list argument passed to this function\n\n\nX_best_seattle_train, X_best_seattle_test, y_seattle_train, y_seattle_test = train_test_split(X_best_seattle, y_seattle, test_size=.30, random_state=42)    # split into train and test\n\nlm_model_seattle = LinearRegression(normalize=True)    # instantiate model\n\nlm_model_seattle.fit(X_best_seattle_train, y_seattle_train)    # fit model\n\ny_seattle_test_preds = lm_model_seattle.predict(X_best_seattle_test)    # predict using the model\n\nscore = r2_score(y_seattle_test, y_seattle_test_preds)      # score the model\n\nprint(score)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# which features have a stronger weighting on the model - SEATTLE - using coefficients?\n\n\n#Use the function\ncoef_df_seattle = coef_weights(lm_model_seattle.coef_, X_best_seattle_train)\n\n#A quick look at the top results\ncoef_df_seattle.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(df_list, index='host_response_time', columns='city2', values='review_scores_rating', aggfunc='mean').sort_values('host_response_time', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(df_list, index='review_scores_cleanliness', columns='city2', values='review_scores_rating', aggfunc='mean').sort_values('review_scores_cleanliness', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Presentation outputs (Visualise)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# need a range of well formatted visuals bringing out the answers to key questions...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(df_list.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cat plot for responding time\n\nsns.set_theme(style='whitegrid')\n\ng = sns.catplot(\n        data=df_list, kind='bar',\n        x='host_response_time', y='review_scores_rating', hue='city2',\n        order=['within an hour', 'within a few hours', 'within a day', 'a few days or more'],\n        ci='sd', palette='dark', alpha=.6, height=6, aspect=1.5,\n        )\n\ng.despine(left=True)\ng.set_axis_labels(\"\", \"Review_Scores_Rating (out of 100)\")\ng.legend.set_title(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# heatmap of correlations selected\n\ndf_list_clean_subset = df_list_clean[['review_scores_value', 'review_scores_cleanliness', 'review_scores_communication', 'review_scores_rating']]\n\n\nf, ax = plt.subplots(figsize=(15,9))\nsns.heatmap(df_list_clean_subset.corr(),\n            annot=True,\n            fmt='0.2f',\n            linewidths=.6,\n            ax=ax,\n           )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleanliness scores versus overall score by city\n\nsns.set_theme(style='white')\n\nsns.relplot(\n            x=df_list['review_scores_cleanliness'],\n            y=df_list['review_scores_rating'],\n            hue=df_list['city2'],\n            size=df_list['number_of_reviews'],\n            sizes=(3,300),\n            alpha=0.5,\n            palette='muted',\n            height=10,\n            data='df_list',\n            )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions"},{"metadata":{},"cell_type":"markdown","source":"## Question 1\n### What actions can a host take to achieve a better overall rating score?\nThere are many things hosts can do that should improve their overall rating score.  Aside from the obvious 'get better ratings on all the sub-categories' (see Question 2) there are clear actions that can be taken that should make a difference.\n\n#### Three very simple things that stand out from the data:\n* Respond to your guests as soon as you can - certainly within one day.  For hosts responding within 'a few days or more' there is a clear penalty in the overall review scores.\n* Keep your calendar up to date - this small task has a high predictive value on the overall rating score.  Leave your calendar for 4 months between updates and you are likely to be penalised in the reviews, whereas if you have updated it in the last two days it is likely that your review score will be better.  Why this is the case is uncertain, but could well be linked to your overall levels of communication and accuracy which give guests a better overall experience.\n* Verify your identity and require your guests to verify their phone number.  These two items both link strongly with a higher overall review score and are both very simple housekeeping tasks to take care of.  Perhaps these are also linked to making sure the overall communication is stronger between host and guest.\n\n\n\n\n## Question 2\n### From the sub-category review scores, which ones have the strongest impact on overall rating score?\nFrom the review sub-category scores, the strongest influencer of overall review scores rating seems to be for 'value'.  This suggests a host needs to get their pricing and their offering in line with what visitors expect for accommodation that they would rate as being 'great value'.  It has the highest coefficient for predicting the overall review scores rating.  \n\nThe second highest predictor in terms of sub-categories is the 'cleanliness' score. A very simple thing to get right, and clearly reflected in the overall rating for the property.\n\nClose behind, in third position, is 'communication'.  Another simple one for hosts to take action and improve, for a better overall rating score.\n\n\n\n\n## Question 3\n### How do the drivers of overall rating differ by city between Seattle and Boston?\nLooking at the two cities independently, the overall key drivers were mostly the same.  The same message of: 'communicate with your guests', 'keep your property (and your Airbnb profile) clean' and 'offer good value' should help drive better ratings in both cities.\n\nA few things stand out as different between the cities:\n* The penalty seems to be higher in Seattle for slow responsiveness in communication.\n* For all 'cleanliness' scores below the best possible rating of 10, Boston seems to have a higher penalty with lower overall ratings than Seattle for the lower cleanliness scores."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}